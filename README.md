# Migration & Improvement Spec: Tesseract â†’ PaddleOCR

.venv\Scripts\activate

python app.py

These arenâ€™t critical now but great upgrades later:

ðŸ¤– Fine-tune a YOLO model just for manga speech bubbles

ðŸ§  Train PaddleOCR with manga fonts

ðŸ’¬ Add subtitle-mode: show all translations in corner

ðŸ”„ Auto-translate when page changes using hash/frame diff

âœ… D. UI / OVERLAY IMPROVEMENTS
You're very close to Google Lens-level polish. Try these:

1. Add dynamic font resizing
Make the font size adapt to bubble height.

Auto-wrap if translated_text.length * font_size > width.

2. Word-level translation preview (optional)
Show OCR text briefly before translating.

Might help if you want users to optionally skip mistranslations.

3. Outline / drop shadow text for readability
In Pillow:

python
Copiar
Editar
draw.text((x+1, y+1), text, font=font, fill="black")  # shadow
draw.text((x, y), text, font=font, fill="white")      # main

âœ… E. PERFORMANCE BOOST
1. Avoid resizing twice
Right now you're resizing before YOLO + OCR.

âœ… Use original for YOLO

âœ… Resize only for OCR after cropping bubbles

2. Multithread OCR and Translation
OCR and translation can run in parallel per bubble:

Use concurrent.futures.ThreadPoolExecutor

Run extract_text_from_bubbles() and translate_batch() in parallel

4. Performance & Responsiveness
Parallelize OCR & Translation with concurrent.futures.ThreadPoolExecutor so multiple bubbles are processed truly in parallel (your GPU can batch OCR, and translation calls can fire off concurrently).

Cache translations (in a dict keyed by original Japanese text) so repeated bubbles (or repeated presses) donâ€™t reâ€‘translate the same string.

5. UI Polish
Dynamic font sizing: Measure each bubbleâ€™s width/height and pick a font size that maximizes legibility without overflow.

Dropâ€‘shadows or outlines behind text to improve contrast on noisy backgrounds.

Smooth fadeâ€‘in animations for the overlay so it doesnâ€™t â€œpopâ€ abruptly.

6. Translation Engine Options
Swap out Google Translate for an offline engine (e.g. argos-translate or a local LibreTranslate server).

Or try the DeepL unofficial API for higher fidelity, then fall back to Google/LibreTranslate if itâ€™s down.

7. Liveâ€‘Video Mode
Instead of â€œone shot per F8 press,â€ hook into a continuous capture loop (e.g. tkinter.after or a background thread) to process frames at, say, 2â€“3â€¯FPS.

Keep your YOLO model loaded once, and reuse it on each frame for near realâ€‘time â€œmangaâ€‘lensâ€ translation.

8. Bubbleâ€‘Detector Improvements
Fineâ€‘tune your YOLO model on your own manga pages for better recall/precision.

Or experiment with Transformerâ€‘based detectors (e.g. DETR) if you need higher accuracy on weird layouts.

## ðŸŽ¯ Goals
1. Replace all Tesseract calls with PaddleOCR (GPU-compatible, angle/vertical text support).  
2. Maintain existing input/output interfaces (e.g. image paths, screenshots).  
3. Add automatic orientation detection (`use_angle_cls=True`) to handle vertical Japanese.  
4. Improve performance: batch processing, GPU support, multithreading as needed.  
5. Plug in translation step after OCR.  
6. Structure code into clear modules:  
   - capture.py    â†’ screenshot / image loading  
   - ocr.py        â†’ text detection & recognition (PaddleOCR)  
   - translate.py  â†’ calls translator API or local model  
   - ui_overlay.py â†’ draws translated text onto images / GUI  
   - main.py       â†’ ties it all together, CLI or hotkey trigger  

## ðŸ“¦ Dependencies
```bash
pip install paddlepaddle paddleocr
pip install deep-translator        # or transformers[torch] for MarianMT
pip install mss keyboard pillow    # screenshot + input + image handling
pip install opencv-python          # overlay drawing
ðŸ§© Module: ocr.py
Initialize:

python
Copiar
Editar
from paddleocr import PaddleOCR
ocr = PaddleOCR(use_angle_cls=True, lang='japan')  # auto-detect rotation/vertical
Function: def extract_text(image: np.ndarray) -> List[Tuple[str, Tuple[int,int,int,int]]]:

Input: BGR/GRAY image array

Call ocr.ocr(image, cls=True)

Flatten results to (text, (x1,y1,x2,y2)) for each line.

Return list of (text, bounding box).

âš™ï¸ Module: translate.py
Support: GoogleTranslator (deep-translator) or local MarianMT

Function: def translate_batch(texts: List[str]) -> List[str]:

Detect source language if needed

Translate in bulk to target (e.g. English)

Handle API errors / rate limits gracefully.

ðŸ–¼ Module: ui_overlay.py
Function: def draw_translations(image: np.ndarray, data: List[Tuple[str, str, bbox]]):

For each (orig, trans, bbox), draw translucent box + text above/beside

Use OpenCVâ€™s putText, rectangle, and optional font scaling.

Consider dynamic font size based on box height.

ðŸ“¸ Module: capture.py
Function: def grab_region(region: Tuple[int, int, int, int] = None) -> np.ndarray:

Use mss or pyautogui.screenshot()

Convert to OpenCV format

Accept optional region or full screen.

ðŸš€ Module: main.py
Parse CLI args (e.g. --gpu, --lang ja, --region 0,0,800,600)

Optionally run in hotkey loop (keyboard.add_hotkey('ctrl+shift+L', process_screen))

Workflow:

img = capture.grab_region()

ocr_data = ocr.extract_text(img)

orig_texts, boxes = unzip(ocr_data)

translations = translate.translate_batch(orig_texts)

result_img = ui_overlay.draw_translations(img, zip(orig_texts, translations, boxes))

cv2.imshow('Lens', result_img) or save to disk

ðŸ”§ Performance & Reliability
GPU Setup: detect paddlepaddle-gpu, fall back to CPU if unavailable.

Batch OCR: if processing many small crops, send to PaddleOCR in one call.

Error Handling: wrap OCR & translation calls in try/except, log failures.

Logging: use logging module, adjustable verbosity (--debug).

Config File: support a JSON/YAML to tweak use_angle_cls, translator API keys, font settings.